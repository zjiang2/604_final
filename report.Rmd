---
title: "Predicting County-Level COVID-19 Vaccine Hesitancy"
date: "`r format(Sys.time(), '%B %d, %Y')`"
author: "Prayag Chatha, Josh Jiang, Declan McNamara"
header-includes:
   - \usepackage{float}
output: 
  pdf_document:
    number_sections: true
    fig_crop: no
urlcolor: blue
bibliography: references.bib
---

```{r setup, include=FALSE}
library(ggplot2)
library(tidyverse)
library(gridExtra)
library(adabag)
library(ROCR)
library(viridis)
library(ggpubr)
library(ggthemes)
library(usmap)

# set chunk default
knitr::opts_chunk$set(
  echo = FALSE,  # don't print the code chunk
  warning = FALSE,  # don't print warnings
  message = FALSE,  # don't print messages
  fig.width = 6,  # set default width of figures
  fig.height = 4,  # set default height of figures
  fig.align = "center",  # always align figure in center
  fig.pos = "H")  # always plot figure at the exact location of the code
source("code/helpers.R")
# read in main data set

# set theme of ggplots (looks a lot cleaner than base config)
theme_set(theme_bw())

no_legend_theme <-theme_tufte() + 
  theme(plot.title = element_text(size = 10),
        legend.position = "none")

legend_theme <-theme_tufte() + 
  theme(plot.title = element_text(size = 10))

rotate_theme <- theme_tufte() + 
  theme(plot.title = element_text(size=16), 
        axis.text.x = element_text(angle = 90),
        legend.position = "none")

rotate_theme_bw <- theme_bw() +
  theme(axis.text.x = element_text(angle = 90),
        legend.position = "none")
  

map_theme <- theme(axis.line=element_blank(), panel.grid.major=element_blank(),
                   axis.text.x=element_blank(), axis.text.y=element_blank(),
                   axis.ticks=element_blank(), axis.title.x=element_blank(),
                   axis.title.y=element_blank(), panel.grid.minor=element_blank())
```

# Introduction

As of December 2021, about 59% of the US population has been fully vaccinated against COVID-19. Despite America's wealth, advanced medical sector, and its creation of the first vaccine, America ranks just 19th in the world in terms of vaccination rates.[^1] Given the abundance of domestic vaccine doses, it is reasonable to assume that large segments of the American populace are unwilling to take the vaccine. Media narratives attribute this public hesitancy to many causes, ranging from political polarization to consumer anxiety about hidden costs.


[^1]: <https://ourworldindata.org/covid-vaccinations>

In this study, we examine the relationship between county-level vaccination rates and local demographics, such as racial composition, economic indicators, and political tendency. A county-level study can reveal interesting geographic patterns by aggregating the behavior of individuals who tend to belong to the same social unit (e.g. socioeconomic class or race). A quantitative analysis of what factors predispose communities toward or against vaccination may be useful in crafting effective public health messaging. 

# Data

```{r loadData}
df_main <- read.csv("data/combined_county_info_0620.csv")
df_main <- clean_data(df_main)

# get list of fips with state and county
dfips <- maps::county.fips %>%
  as_tibble %>% 
  extract(polyname, c("region", "subregion"), "^([^,]+),([^,]+)$")

# append fips to map data and df_main data
dall <- map_data("county") %>% 
  left_join(dfips) %>%
  left_join(df_main, by=c("fips"="FIPS"))
```

## Data Sources

We derived our data on county-level vaccination rates[^2] from a scrape of the CDC's [county view dashboard](https://covid.cdc.gov/covid-data-tracker/#county-view) dating back to June 20th, 2021. We chose this date as roughly half of the US population had been fully vaccinated by this point in time.[^3]. For potential predictors of county-level vaccination, we obtained county-level results for the 2020 US Presidential election[^4], demographic data (e.g. racial composition, income levels) from the 2015 5-year Census estimate[^5], as well as county education levels (i.e. prevalence of a bachelor's degree or higher) [^6].

[^2]: <https://github.com/covidestim/cdc-vaccine-data>

[^3]: <https://ourworldindata.org/covid-vaccinations?country=USA>

[^4]: <https://www.kaggle.com/etsc9287/2020-general-election-polls>

[^5]: <https://www.kaggle.com/muonneutrino/us-census-demographic-data>

[^6]: <https://www.ers.usda.gov/data-products/county-level-data-sets/>

## Data Overview

Our combined data set contains complete data on 2,809 counties[^7] in 47 states (sans Alaska, Hawaii, and Texas) as well as Washington, DC. These counties ranged from metropolitan areas[^8] such as Los Angeles, Cook, and Maricopa counties to rural areas containing only a few hundred inhabitants. As counties primarily represent geographic area, and people tend to be clustered in urban areas, most counties are less populated than average.

[^7]: Out of the approximately 3,100 counties and equivalent municipalities in the USA

[^8]: In New York City, vaccination data for all five boroughs were combined, which led to its exclusion from the final data set.

These counties had a mean and median adult vaccination rate of 41% as of June 20th, 2021, with most counties falling within a range of about 30-50% vaccination. We inspected several counties with extremely high or low vaccination levels.

```{r hesitancyOverview, results="hide", fig.width=6, fig.height=3, fig.cap="Map and Distribution of 18-Plus vaccination Rates by County, June 20th, 2021"}
pp1 <- dall %>% 
  ggplot(aes(x=long, y=lat, group = group)) +
  geom_polygon(aes(fill=vax_rate_18_plus), color=NA) +
  scale_fill_gradientn(colors=c("red","orange","green","blue"), na.value="grey") +
  labs(fill = "Vax rate %") + 
  theme(text = element_text(size=5)) + map_theme

pp2 <- ggplot(df_main, aes(x=vax_rate_18_plus)) +
  geom_histogram() + xlab("% Fully Vaccinated, 18+")

grid.arrange(pp1,pp2, nrow=2, layout_matrix=matrix(c(1,1,2), nrow=1))
```

### Counties with Unusually High Vaccination Rates

The four counties with the highest vaccination rates--McKinley NM (100%), Chattahoochee GA (100%), Martin NC (86%), and Santa Cruz AZ (85%)--are all rural areas with large minority (Native American, Black, or Hispanic) populations. Also near the top of the list were several affluent counties, such as Montgomery MD, Marin CA, and Glacier MT, which all had a vaccination rate of 80% or more. The former group could indicate an effect where counties that were particularly hard-hit by Covid-19 (i.e., poor and less-white areas) have seen widespread vaccination adoption. The latter group suggests that wealth correlates with vaccination willingness. In general, the counties with the highest vaccination rates generally leaned Democratic in the 2020 presidential election.

### Counties with Unusually Low Vaccination Rates

The counties with the lowest vaccination rates (0-2%) were mostly in rural (i.e. western) Virginia, such as Appomattox County (1.7%) and the city of Lynchburg (0.8%). Other counties at the bottom end of the range included the island of Nantucket, MA (1.2%) and Morgan County, WV (2.1%). Remarkably, the 50 counties with the lowest vaccination rate (6% or lower) are all located in either Virginia, Georgia, West Virginia, or the Cape Cod region of Massachusetts. These include both mostly-white and white-minority counties, though most of them leaned Republican in the 2020 presidential election. In today's politicized climate, Republicans may be more likely to be skeptical of a vaccine rollout happening under a Democratic administration.

### Geographic Patterns

Figure 1 shows a map of vaccination rates in the lower 48 states (minus Texas). The southern states West Virginia, Virginia, and Georgia are home to many counties with some of the lowest vaccination rates. The Great Plains (e.g Nebraska, North Dakota) appear to have lower-than-average vaccination adoption, though rural counties in neighboring states Minnesota and Iowa tend to be widely-inoculated. The Northeast, the West Coast, and southern Florida (notably home to many seniors) are some other areas with widespread vaccination.

## Exploratory Data Analysis

Another view of our data set involves comparing all counties' vaccination rates based on single predictors. Figure 2 shows scatterplots of vaccination against six features roughly corresponding to education, wealth, political tendency, ethnicity, economic class, and population density: we speculated that these features have relatively high variance across American counties.

We see that an educated and high-earning populace tends to predict a high vaccination rate, both having a correlation coefficient of $\rho = 0.41$. In contrast, the more a county supported Donald Trump in the 2020 Presidential Election, the lower vaccination rates tend to be ($\rho = - 0.38$). We speculate that a college education would be associated with a trust of scientific and media institutions (and thus amenability to vaccination). Despite an abundance of free vaccine doses, poorer Americans may be less able to miss work for a shot or else fear hidden costs (especially if they lack health insurance). It is unsurprising that political leanings correlate so strongly with vaccination rates given that attitudes towards public health measures (e.g. mask wearing, social distancing) have been politicized since nearly the start of the pandemic.

Of the several variables pertaining to a county's racial composition, the percent of Black population, had the strongest relationship with vaccination rates, showing a somewhat negative correlation ($\rho=-0.25.$).[^9] Similarly, the percentage of the workforce engaged in production (i.e., industrial or agricultural work) was a negative predictor ($\rho=-0.20.$) These suggest some disparities in either access to (or trust of) inoculation efforts. We also observed that (log-scaled) population was associated with vaccination ($\rho = 0.33$), suggesting more vaccination in urban areas. More populated counties (i.e. urban America) tended to vote for Joe Biden in the last election, so this effect could be merely a reflection of existing political polarization.

[^9]: Given the history of [unethical medical experiments](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) run on Black Americans, racialized attitudes towards the Covid-19 vaccine is not surprising. In contrast, the percentage of white population was largely uncorrelated with vaccination.

```{r correlations, fig.width=6, fig.height=4, fig.cap="Correlation Scatterplots for Select Demographic Features"}
plot_color <- alpha("blue", 0.6)
size = 0.4
p1 <- ggplot(data=df_main, aes(x=pct_college, y=vax_rate_18_plus))  + geom_point(color=plot_color, size=0.4) + 
   xlab("% College Educated") + ylab("Adult Vacc. Rate")
p2 <- ggplot(data=df_main, aes(x=IncomePerCap, y=vax_rate_18_plus))  + geom_point(color=plot_color, size=0.4) + 
  xlab("Income per Capita ($)") + ylab("Adult Vacc. Rate")
p3 <- ggplot(data=df_main, aes(x=percentage20_Donald_Trump, y=vax_rate_18_plus))  + geom_point(color=plot_color, size=0.4) + 
   xlab("% Trump Vote, 2020") + ylab("Adult Vacc. Rate")
p4 <- ggplot(data=df_main, aes(x=Black, y=vax_rate_18_plus))  + geom_point(color=plot_color, size=0.4) + 
  xlab("% Black") + ylab("Adult Vacc. Rate")
p5 <- ggplot(data=df_main, aes(x=Production, y=vax_rate_18_plus))  + geom_point(color=plot_color, size=0.4) + 
   xlab("% Prod. Workers") + ylab("Adult Vacc. Rate")
p6 <- ggplot(data=df_main %>% transform(LogPop = log(TotalPop)), aes(x=LogPop, y=vax_rate_18_plus))  + geom_point(color=plot_color, size=0.4) + 
  xlab("Log Population") + ylab("Adult Vacc. Rate")
grid.arrange(p1, p2, p3, p4, p5, p6, nrow=2, ncol=3)
```

# Classifying Low and High Vaccination Counties

```{r createTargetVariable}
pred_threshold <- 44
df_main$y <- as.integer(df_main$vax_rate_18_plus >= pred_threshold)
```

The outcome variable we aim to model is the proportion of fully-vaccinated adults. As binary classification is a more tractable problem than regression (especially on the 0-1 range), we framed our task as identifying vaccine-hesistant and vaccine-willing counties. Hence, we split counties into two categories: 1,646 low vaccination counties having a rate less than 44%, and 1,163 high vaccination counties with a rate greater than or equal to 44%. This resulted in a 59-41 split in the data. It is worth noting that about 70 million Americans live in "low" counties and 205 million Americans live in "high" counties.

We compare the performance of three classification models: AdaBoost, Random Forests, and Logistic Regression. We use a 75-10-15 split for training, validation, and testing sets. The purpose of the validation set is to select parameters that generalize well to unseen data with minimal overfitting, while the final performance is assessed on a previously-unseen test data set.

## AdaBoost

AdaBoost, short for "adaptive boosting," is the classical[^10] boosting algorithm for classification. The philosophy of boosting is that an ensemble of weak learners can act in concert as a strong learner. At each iteration of training, AdaBoost assigns additional weight to previously-misclassified observations, which helps it learn to correct its own mistakes. AdaBoost makes relatively few assumptions about the data, making it a good "out-of-the-box" classifier.

[^10]: Created by Freund and Schapire in 1997.

```{r adaBoost}
set.seed(1160)
select_cols <- c("y", "pct_college", "TotalPop", "Black", "Production",
                                "IncomePerCap", "Poverty", "Walk", "percentage20_Donald_Trump")
df_quant <- df_main %>% select(select_cols)
df_quant$TotalPop <- log(df_quant$TotalPop)
cci <- split_data(df_quant)
cci$train$y<- as.factor(cci$train$y)

n_trees = 1:20
L = length(n_trees)
train_aucs = rep(0, L)
val_aucs = rep(0, L)

for (i in 1:L) {
  ab_model <- boosting(y ~ ., 
                       data=cci$train, mfinal=n_trees[i])
  train_probas <- ab_model$prob[, 2]
  train_pred <- prediction(train_probas, cci$train$y)
  train_auc.tmp <- performance(train_pred, "auc")
  train_aucs[i] <- as.numeric(train_auc.tmp@y.values)
  
  val_probas <- predict.boosting(ab_model, cci$validate)$prob[, 2]
  val_pred <- prediction(val_probas, cci$validate$y)
  val_auc.tmp <- performance(val_pred, "auc")
  val_aucs[i] <- as.numeric(val_auc.tmp@y.values)
}
```

```{r adaConvergence, fig.width=5, fig.height=3, fig.cap="AdaBoost AUC Score Convergence"}
ada_conv <- data.frame(n_trees <- n_trees,
                       train_auc <- train_aucs,
                       val_auc <- val_aucs)
ggplot(data = ada_conv) + geom_line(aes(n_trees, train_auc, colour="Training")) + 
  geom_line(aes(n_trees, val_auc, colour="Validation")) +
  ylim(0.7, 1) + scale_x_continuous(name = "Number of Trees", breaks=n_trees) +
  scale_color_manual(name = "Data Set", values=c("Training" = "darkgreen", "Validation" = "orange")) +
  ylab("AUC") + labs(title="Convergence of AdaBoost Classifier")

```

We trained our AdaBoost model on the following county-level demographic features: (1) percent college educated, (2) total population, (3) percent Black, (4) percent working in production, (5) income per capita, (6) poverty rate, (7) percent of walking commuters, and (8) Trump 2020 vote share. We denote the number of trees in Adaboost, the algorithm's main parameter, as $m$, and plot the AUC scores on training and validation sets as $m$ ranges from 1 to 20. As we would expect, the model overfits the training set somewhat. As the number of trees increases past twelve, validation AUC seems to oscillate around a plateau, whereas training AUC keeps increasing. We decided to set $m=6,$ as validation AUC drops slightly at $m=7,$ and overfit starts getting significantly worse for larger $m.$ This relatively simple model achieved an average accuracy of 82.9% on the test data set as well as an AUC score of 0.90.

The AdaBoost model depended mostly on the 2020 election results, percentage of Black population, and income per capita in that order, these features accounting for 76% of overall variable importance. The table below shows the importance percentage for each of the eight predictors.

```{=tex}
\begin{table}[htp]
\centering
\caption{AdaBoost Feature Importance}
\begin{tabular}{|l|l|}
\hline
Feature             & \% Importance \\ \hline
\% Trump Vote       & 36.0          \\ \hline
\% Black            & 23.7          \\ \hline
Income per Capita   & 16.5          \\ \hline
Poverty Rate        & 6.9           \\ \hline
Total Population    & 6.5           \\ \hline
\% College Educated & 4.9           \\ \hline
\% Walking to Work  & 4.2           \\ \hline
\% Prod. Workers    & 1.3          \\ \hline
\end{tabular}
\end{table}
```

```{r evaluation}
set.seed(1160)
final_ab_model <- boosting(y ~., data=cci$train, mfinal=6)
test_probas <- predict.boosting(final_ab_model, cci$test)$prob[, 2]
test_pred <- prediction(test_probas, cci$test$y)
test_auc.tmp <- performance(test_pred, "auc")
test_auc <- as.numeric(test_auc.tmp@y.values)
test_acc <- mean(as.integer((test_probas >= 0.5) == cci$test$y))
```

## Random Forest

In recent years, tree-based methods have exploded in popularity due to their flexibility, ability to learn non-linear relationships, and excellent prediction accuracy. As a non-parametric classification algorithm, random forest in particular makes no formal distributional assumptions about the data while allowing for ensemble learning. Furthermore, random forests have a natural way to rank the importance of variables. The variable importance is determined by the mean difference in out-of-bag prediction error between the training data set and the test data set with said variable permuted; the score is normalized by the standard deviation of the differences. We used the `randomForest` package in R to implement the algorithm.

```{r, fig.cap="Out-of-bag Error Rate for Random Forest Classifier.",fig.height=3,fig.width=5}
library(randomForest)
set.seed(12)

# get oob error rates for various number of trees
cci_rf <- split_data(df_main[,-c(2:6,8)])
tune_df <- c()

for (n in 1:20*20){
  rf_tmp <- randomForest(factor(y)~.,data=cci_rf$train[,-c(1)], ntree=n)
  oob_err <- rf_tmp$err.rate[n,1] %>% as.numeric()
  tune_df <- rbind(tune_df, c(n,oob_err))
}

tune_df <- tune_df %>% as.data.frame()
colnames(tune_df) <- c("n_trees","oob_err")

rf_pp1 <- tune_df %>%
  ggplot(aes(x=n_trees, y=oob_err)) +
  geom_point() + geom_line() +
  xlab("Number of Trees") + ylab("Out-of-bag Error")

rf_pp1
```

Similarly to AdaBoost, the random forest algorithm trains an ensemble of weak classifiers to become a strong classifier. We chose the number of trees in the forest, $B$, to be the one that minimizes out-of-bag (OOB) error. We decided to use $B=240$, although we observed that OOB error seems to bottom out after $B=100$. The algorithm was able to achieve a 85.5% prediction accuracy on the test set.

```{r, fig.cap="Feature importance for random forest classifier."}
# run rf using optimal number of trees
n <- tune_df$n_trees[which.min(tune_df$oob_err)]
rf <- randomForest(factor(y)~.,data=cci_rf$train[,-c(1)], ntree=n, importance=TRUE)

# calculate prediction accuracy of rf
rf_pred <- predict(rf, newdata=cci_rf$test[,-c(1,47)]) %>% as.numeric()
rf_acc <- pred_acc(cci_rf$test$y, rf_pred-1)

# get variable importance
var_imp <- importance(rf, type=1) %>% as.data.frame() %>%
  rownames_to_column(var="Variable") %>%
  mutate(Variable = fct_reorder(Variable, MeanDecreaseAccuracy, .desc=TRUE))

rf_pp2 <- var_imp %>% ggplot(aes(x=Variable, y=MeanDecreaseAccuracy)) +
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=5))

rf_pp2
```

The random forest classifier identified four major factors using the mean decrease in prediction accuracy criteria. These four are the proportion of people who voted for Joe Biden and Donald Trump in 2020, income per capita, and proportion of people who are Black. Clearly, the source of vaccine hesitancy is multifaceted. The four major factors show that political views and socioeconomic status play roles in mistrust of the vaccine.

## Logistic Regression

We explored whether or not a parametric model could accurately explain our data using logistic regression. This model assumes that the log odds of the probability that a given county is vaccine-accepting can be written as a linear combination of our predictor variables, i.e. \begin{equation}
\log \frac{P(Y = 1)}{1-P(Y=1)} = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p.
\end{equation}

The parametric constraint may not accurately describe our data, so it will be interesting to examine how well this model performs compared to more "black-box" models like Random Forest and Adaboost, both of which make no implicit assumptions on the data generating process.

We have many degrees of freedom in the choice of which subset of predictors $(X_1, \dots, X_p)$ to use in our model. Rather than choosing a subset arbitrarily or naively (e.g., the set of all predictors), we adopt a principled approach using the forward stepwise and backward stepwise model selection process, implemented in R. The forward stepwise approach begins with the empty (intercept-only) model for the log odds, and at each step adds in the predictor that results in the largest decrease in model AIC, terminating when no further improvement is achieved. The backward stepwise process does the same, but begins with the full model (every predictor variable) and removes a predictor at each step to increase the AIC. Additionally, we center and scale all predictors prior to the model fitting and selection process so that the coefficients $\beta_i$ will be on comparable scale.

```{r}
big_train <- data.frame(rbind(cci_rf$train, cci_rf$validate))[,-1]
test <- data.frame(cci_rf$test)[,-1]

big_train <- data.frame(cbind(scale(big_train[,-ncol(big_train)]), big_train$y)) %>%
  rename(y = V45)
test <- data.frame(cbind(scale(test[,-ncol(test)]), test$y)) %>%
  rename(y = V45)
```

```{r}
# Perform forward and backward stepwise
empty_mod <- glm(y ~ 1, data=big_train, family = binomial)
full_mod <- glm(y ~ ., data=big_train, family=binomial)
fs_results_forward <- step(empty_mod, scope=list(lower=empty_mod, upper=full_mod), direction = 'forward', trace=0)
fs_results_backward <- step(full_mod, scope=list(lower=empty_mod, upper=full_mod), direction = 'backward', trace=0)
```

After running both procedures, we can examine the variables included in each model. Below, we plot the AIC of the forward stepwise model at each step, along with the predictor added at each step. We see that the final model chosen by this procedure includes 19 total predictors.

```{r, fig.cap="Forward stepwise procedure for logistic regression."}
order <- names(fs_results_forward$R[,1])

ggplot(data=fs_results_forward$anova,
       aes(x=factor(order, levels=as.character(order)), y=AIC, group=1)) +
  geom_line() +
  xlab('Variable Added') +
  ylab('AIC') +
  ggtitle('Forward Stepwise Procedure - Logistic Regression') +
  rotate_theme_bw
```

Perhaps unsurprisingly, the variables added to the model first seem to match with the feature importances we've observed thus far from our other methods. Income per capita, percentage of votes for Donald Trump in 2020, and percentage of black population are the first three predictors added, matching three out of the top four predictors for the random forest.

It's less informative to examine a similar plot for the backward stepwise procedure, as we would see the variables removed rather than those which remain. Instead, we examine the plots for the magnitude of the logistic regression coefficients for each of the two models. For the forward stepwise model, predictors are plotted in the order in which they were added to the model. For the backward stepwise model, the predictors are in arbitrary order.

```{r, fig.height=8, fig.cap="Model coefficients for the forward stepwise logistic regression model (top) and backward stepwise (bottom)"}
mod_forward <- glm(fs_results_forward$formula, data=big_train, family=binomial)
mod_backward <- glm(fs_results_backward$formula, data=big_train, family=binomial)

df_f <- data.frame(cbind(names(mod_forward$coefficients), as.double(mod_forward$coefficients)), as.factor(sign(mod_forward$coefficients)))[-1,] %>%
  rename(Variable = X1,
         Coefficient = X2,
         Sign = 3) %>%
  mutate(Variable = factor(Variable, levels=as.character(Variable)),
         Coefficient = as.double(Coefficient),
         Sign = as.character(Sign))

forward_plot <- ggplot(df_f, aes(x=Variable, y=Coefficient)) +
  geom_bar(aes(fill=Sign), stat = 'identity') +
  scale_fill_viridis(discrete = T) +
  rotate_theme_bw
  #scale_fill_manual(values=c('red', 'blue')) +

df_b <- data.frame(cbind(names(mod_backward$coefficients), as.double(mod_backward$coefficients)), as.factor(sign(mod_backward$coefficients)))[-1,] %>%
  rename(Variable = X1,
         Coefficient = X2,
         Sign = 3) %>%
  mutate(Variable = factor(Variable, levels=as.character(Variable)),
         Coefficient = as.double(Coefficient),
         Sign = as.character(Sign))

backward_plot <- ggplot(df_b, aes(x=Variable, y=Coefficient)) +
  geom_bar(aes(fill=Sign), stat = 'identity') +
  scale_fill_viridis(discrete = T) +
  rotate_theme_bw
  #scale_fill_manual(values=c('red', 'blue')) +

ggarrange(forward_plot, backward_plot,
          nrow=2,
          ncol=1)
```

Because the data are standardized, the scale of the coefficients are comparable, allowing us to examine their importance in the prediction problem. All other predictors being equal, larger coefficients in magnitude indicate a feature which has a higher impact on the log odds of a county being vaccine hesitant. Large positive coefficients indicate that a county with higher values for a particular predictor are more likely to be vaccine-accepting; large negative coefficients indicate that large values for that predictor are likely to indicate a county that is vaccine-hesitant. We caution being overly simplistic in interpreting these numbers, though; due to explaining-away and correlations among the features, some results may be unintuitive.

Keeping this interpretation in mind, the results match intuition reasonably well, particularly politically. One might expect counties with larger share of Democratic voters (higher values percentage of votes to Joe Biden in 2020) to be less vaccine-hesitant, and indeed this is the case. A large positive coefficient on this predictor in the backward model indicates that a county with a higher than usual value for this field is more likely to accepting of the vaccine, all other predictors being equal. Perhaps expectedly, we see a large negative coefficient on the percentage of votes for Donald Trump in 2020 in the forward model which matches the intuition that the coefficients on these two predictors should be different signs. 

We mus be careful not to read too much into these coefficients, though, as they should be considered as the impact of the predictor \textit{after controlling for the effects of the others}. Because of this explaining-away effect, several predictors have unexpected signs. Percentage of college-educated individuals has negative coefficients in both models, going against intuition that education and wealth lead to vaccine acceptance. In both models, rates of cases and deaths have negative coefficients, although this may be a chicken and egg problem: it seems more likely that these rates tend to be higher in some counties \textit{because} their vaccination rates are lower, rather than causality going the other direction.

As far as what other variables are included in each model, we sense a theme of urban vs. rural living. Predictors like poverty, mean commute, percentage who are self-employed, percentage who work in an office environment, etc. all seem to vary with the underlying urban/rural divide, a dichotomy which may explain the usefuless of many of these features for the model. 

Having fit these two models on our training data, we can examine their test accuracy. On the test set, the logistic regression model fit with the forward stepwise procedure had a test accuracy of 84.6%, while the model fit with the backward stepwise procedure had a test accuracy of 85.3%. The similarity of the two accuracies suggests that the two models are substantially similar, even though they do differ in a few predictors.

```{r, include=FALSE}
preds_forward_mod <-ifelse(predict(mod_forward,
                                   test,
                                   type='response') > 0.5, 
                           1, 
                           0)
preds_backward_mod <- ifelse(predict(mod_backward,
                                   test,
                                   type='response') > 0.5, 
                           1, 
                           0)

lr_forward_accuracy <- mean(preds_forward_mod == test$y)
lr_backward_accuracy <- mean(preds_backward_mod == test$y)

```

## Model Comparison

The three models were trained on the same training set. To assess their performances, we used the same test set for each model.

```{=tex}
\begin{table}[htp]
\centering
\caption{Mean Accuracy on Test Data}
\begin{tabular}{|l|l|}
\hline
Model               & Test Accuracy \\ \hline
AdaBoost            & .829          \\ \hline
Random Forest       & .855          \\ \hline
Logistic Regression & .853          \\ \hline
\end{tabular}
\end{table}
```

Table 2 shows the prediction accuracies on the test set for the three classification models. Since logistic regression produces a linear decision boundary while random forest can learn more complex ones, we found it surprising that logistic regression performed on par with random forest. The relatively poor performance of AdaBoost is likely due to it only training on a subset of the predictors.

# Post-Hoc Analysis

Since random forest is the best classification model considered, we will perform some post-hoc analysis on its performance. In particular, we are interested in identifying geographic patterns in the model's classification error. To allow ourselves as complete a map of the US as possible, we decided to use the entire data set in this analysis. For a given observation $x_i$ in the training set, we predicted it using only trees not trained on the observation. Of course, while the results of the training set will not reflect the true capabilities of the random forest, we believe it will be close enough.

```{=tex}
\begin{table}[htp]
\centering
\caption{Confusion matrix for random forest. FP=0.190, FN=0.173.}
\begin{tabular}{|l|l|l|}
\hline
& Vaccine Willing & Vaccine Hesitant \\ \hline
Positive & 861 & 202\\ \hline
Negative & 302 & 1444\\ \hline
\end{tabular}
\end{table}
```

```{r, fig.cap="Map of counties color coded by classification result using random forest."}
# get rf predictions
train_pred <- predict(rf) %>% as.numeric()
test_pred <- predict(rf, newdata=cci_rf$test[,-c(1,47)]) %>% as.numeric()
valid_pred <- predict(rf, newdata=cci_rf$validate[,-c(1,47)]) %>% as.numeric()

# combine predictions, actual, FIPS
res <- cbind(c(cci_rf$train$FIPS, cci_rf$test$FIPS, cci_rf$valid$FIPS),
             c(cci_rf$train$y, cci_rf$test$y, cci_rf$valid$y),
             c(train_pred, test_pred, valid_pred)-1) %>% as.data.frame()
colnames(res) <- c("FIPS","actual","pred")

# plot
ph1 <- dall %>%
  left_join(res, by=c("fips"="FIPS")) %>%
  mutate(result = ifelse(is.na(actual), "NA", 
                  ifelse(actual==1 & pred==1, "true positive",
                  ifelse(actual==0 & pred==0, "true negative",
                  ifelse(actual==1 & pred==0, "false negative",
                         "false positive"))))) %>% 
  ggplot(aes(x=long, y=lat, group = group)) +
  geom_polygon(aes(fill=result), color=NA) +
  scale_fill_manual(values=c("NA"="grey", "true positive"="blue",
                             "true negative"="darkgreen", "false positive"="purple",
                             "false negative"="red")) +
  map_theme

ph1
```

There appears to be a concentration of misclassifications at the border of hesitancy regions. For example, there are many false negatives in parts of Illinois, Indiana, Ohio, Kentucky. Those states are known to be the boundary between the Midwest/Northeast, which are generally vaccine willing regions, and the South, which contains many of the most vaccine hesitant areas.

```{r}
# join original df with results
df_main_res <- df_main %>% left_join(res)

# query top 3 false negatives
fn <- df_main_res %>% 
  filter(actual==1 & pred==0) %>%
  select(FIPS, vax_rate_18_plus, percentage20_Joe_Biden, IncomePerCap, Black) %>%
  slice_max(vax_rate_18_plus, n=3)

# query top 3 false positives
fp <- df_main_res %>% 
  filter(actual==0 & pred==1) %>%
  select(FIPS, vax_rate_18_plus, percentage20_Joe_Biden, IncomePerCap, Black) %>%
  slice_min(vax_rate_18_plus, n=3)
```

In table 4, we looked at the misclassifications on a county level. In particular, we looked at the counties that had the worst misclassifications - highly vaccinated counties that were classified vaccine hesitant and vice versa.

```{=tex}
\begin{table}[htp]
\centering
\caption{Worst misclassifications - false positives and false negatives - and their characteristics.}
\begin{tabular}{l l l l l l}
\hline
 & County & Vax Rate & Joe Biden & Income per Capita & Proportion Black \\ 
 \hline
Worst FP & Nantucket, MA & 0.012 & 0.717 & 45000 & 0.093\\
2nd Worst FP & Roanoke, VA & 0.019 & 0.388 & 31370 & 0.056\\ 
3rd Worst FP & Winchester City, VA & 0.036 & 0.549 & 26182 & 0.104\\ 
\hline
Worst FN & Martin, NC & 0.859 & 0.471 & 19032 & 43.5\\
2nd Worst FN & Santa Cruz, AZ & 0.854 & 0.672 & 17795 & 0.2\\
3rd Worst FN & Graham, KS & 0.750 & 0.171 & 28233 & 0.031\\ 
\hline
\end{tabular}
\end{table}
```

# Stability Check

Having concluded our analysis, we would like to verify that our results are robust to the time frame of data collection. Recall that we only selected a particularly snapshot in time, right around June 20th, 2021, to determine vaccine hesitancy. In this section, we consider a different snapshot in time, the week of April 18th, 2021, and rerun our analysis for the random forest. We will examine whether or not our ability to predict vaccine hesitancy remains stable, and whether or not the variables most relevant to predicting vaccine hesitancy will remain similar as well. Due to increased vaccination rates over time, this analysis calls for a reformulation of how we label the ground truth of "hesitant" or "not hesitant" for each county. We again choose the 60th quantile of vaccination rate as the cutoff value; those counties above this threshold (29.7%) will be labeled as vaccine-accepting, while those below will be labeled vaccine hesitant.

```{r altDataSet}
df_april <- read.csv("data/combined_county_info_0418.csv")
df_april <- clean_data(df_april, drop=FALSE)

# get list of fips with state and county
dfips_april <- maps::county.fips %>%
  as_tibble %>% 
  extract(polyname, c("region", "subregion"), "^([^,]+),([^,]+)$")

# append fips to map data and df_main data
dall_april <- map_data("county") %>% 
  left_join(dfips) %>%
  left_join(df_april, by=c("fips"="FIPS"))
```

```{r}
#Set 60th percentile cutoff.
cutoff_april <- quantile(df_april$vax_rate_18_plus, .60)
df_april$y <- as.integer(df_april$vax_rate_18_plus >= cutoff_april)
```

```{r}
# Rerun random forest
# get oob error rates for various number of trees
cci_rf_april <- split_data(df_april[,-c(2:6,8)])
tune_df_april <- c()

for (n in 1:20*20){
  rf_tmp_april <- randomForest(factor(y)~.,data=cci_rf_april$train[,-c(1)], ntree=n)
  oob_err_april <- rf_tmp_april$err.rate[n,1] %>% as.numeric()
  tune_df_april <- rbind(tune_df_april, c(n,oob_err_april))
}

tune_df_april <- tune_df_april %>% as.data.frame()
colnames(tune_df_april) <- c("n_trees","oob_err_april")

rf_pp1_april <- tune_df_april %>%
  ggplot(aes(x=n_trees, y=oob_err_april)) +
  geom_point() + geom_line() +
  xlab("Number of Trees") + ylab("Out-of-bag Error")

rf_pp1_april
```

We see some differences compared to the OOB error rate when using the snapshot from June, primarily in the error rates. While in June these out-of-bag error rates got down to as low as 18% or so, when we train on the data set from April our error is a bit higher, around 23% at best.

In trying to determine what factors primarily contribute to vaccine hesitancy, consistency across these two timeframes will help convince us that some variables truly are reliable predictors, rather than black box aids for the model. Below, we plot the feature importances across both time frames, June and April, for comparison.

```{r, fig.cap="Comparison of feature importances for random forest classifier.", fig.height=8}
# run rf using optimal number of trees
n <- tune_df_april$n_trees[which.min(tune_df_april$oob_err_april)]
rf_april <- randomForest(factor(y)~.,data=cci_rf_april$train[,-c(1)], ntree=n, importance=TRUE)

# calculate prediction accuracy of rf
rf_pred_april <- predict(rf_april, newdata=cci_rf_april$test[,-c(1,47)]) %>% as.numeric()
rf_acc_april <- pred_acc(cci_rf_april$test$y, rf_pred_april-1)

# get variable importance
var_imp_april <- importance(rf_april, type=1) %>% as.data.frame() %>%
  rownames_to_column(var="Variable") %>%
  mutate(Variable = fct_reorder(Variable, MeanDecreaseAccuracy, .desc=TRUE))

rf_pp2_april <- var_imp_april %>% ggplot(aes(x=Variable, y=MeanDecreaseAccuracy)) +
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=5))

ggarrange(rf_pp2 + ggtitle('Feature Importances - June'), 
          rf_pp2_april+ ggtitle('Feature Importances - April'),
          nrow=2,
          ncol=1)
```

While we see some differences, there are striking similarities as well. Both plots indicate a steep drop-off in importance after the first five predictors, suggesting that these five are most important to model beyond others. In both time frames, we have consistency in four out of the top five predictors: income per capita; percentage of votes for Donald Trump in 2020; percentage of votes for Joe Biden in 2020; and percentage of black residents. While there order are shuffled slightly within the top five importances, seeing consistency in these feature importances is encouraging - it reassures that us our findings are valid rather than happenstance.

We may also be interested in exploring the question of how our predictions change over time for each county. Below, we plot our model predictions from both April and June.

```{r, fig.height=8, fig.cap="Comparison of model predictions."}
# get rf predictions
train_pred_april <- predict(rf_april) %>% as.numeric()
test_pred_april <- predict(rf_april, newdata=cci_rf_april$test[,-c(1,47)]) %>% as.numeric()
valid_pred_april <- predict(rf_april, newdata=cci_rf_april$validate[,-c(1,47)]) %>% as.numeric()

# combine predictions, actual, FIPS
res_april <- cbind(c(cci_rf_april$train$FIPS, cci_rf_april$test$FIPS, cci_rf_april$valid$FIPS),
             c(cci_rf_april$train$y, cci_rf_april$test$y, cci_rf_april$valid$y),
             c(train_pred_april, test_pred_april, valid_pred_april)-1) %>% as.data.frame()
colnames(res_april) <- c("FIPS","actual","pred")

# plot
ph1_april <- dall_april %>%
  left_join(res_april, by=c("fips"="FIPS")) %>%
  mutate(result = ifelse(is.na(actual), "NA", 
                  ifelse(actual==1 & pred==1, "true positive",
                  ifelse(actual==0 & pred==0, "true negative",
                  ifelse(actual==1 & pred==0, "false negative",
                         "false positive"))))) %>% 
  ggplot(aes(x=long, y=lat, group = group)) +
  geom_polygon(aes(fill=result), color=NA) +
  scale_fill_manual(values=c("NA"="grey", "true positive"="blue",
                             "true negative"="darkgreen", "false positive"="purple",
                             "false negative"="red")) +
  map_theme

ggarrange(ph1 + ggtitle('Predictions - June'),
          ph1_april + ggtitle('Predictions - April'),
          nrow=2,
          ncol=1)
```

Overall, the model errors seem consistent. The same counties often seem to be misclassified in both time frames, suggesting that these few may simply be not well explained by the model or predictors in some fashion. Interestingly, though, we see that our model is indeed capable of catching changing behaviors over time. Florida in particular demonstrates this - we see a high level of vaccine hesitancy in April in the form of lots of green counties (vaccine-hesitant counties), which we tend to predict correctly. By June, though, many counties in upper Florida have switched from vaccine-hesitant to not so, indicated by the presence of blue. Interesting, our model is able to follow the change in trends in these counties, despite seemingly relying on some fields which are static over time: percentage of votes for a particular candidate; income levels; demographics. We speculate that our inclusion of COVID-19 related data allowed us to follow changing behavior because this data causes people to change the vaccine views - as counties are hit hard with the pandemic, people in those counties may be more inclined to get the vaccine where they might not have before.

# Conclusion

Across various models and in two separate temporal snapshots, we see that a few county variables are strong predictors of vaccination tendency. The share of votes that Joe Biden and Donald Trump received in the 2020 presidential election are effective indicators of vaccine willingness and hesitancy respectively, speaking to the stark polarization of American society. We have seen that life-or-death public health measures such as mask-wearing, social distancing, and vaccination tend to develop into partisan issues over the past two years. It is an interesting hypothetical to speculate whether Republican-leaning counties would have higher vaccination rates had the pandemic erupted under a Democratic president and the vaccine roll-out happened under a Republican.

Similarly, a county's percentage Black population was a relatively significant feature, with Black counties tending to have lower-than-average vaccine adoption. Counties with a large Black population are mainly in the rural South or urban North (e.g., Wayne County, MI). In the former case, this could be more of an associative effect, as white-majority counties in the South tended to be vaccine hesitant as well. A study of Southern counties in particular, controlling for income and partisan lean, might indicate whether race (i.e., a history of segregation and discrimination) is a cause of vaccine hesitancy or is merely associated with it.

Lastly, all models ascribed importance to variables that measure a county's overall levels of wealth, such as income per capita and the poverty rate. While the vaccine is free, polling suggests that many Americans believe that the vaccine could lead to a surprise medical bill.^[https://www.nytimes.com/2021/06/01/upshot/covid-vaccine-hesitancy-cost.html] Such hidden costs are a much greater threat to Americans without savings or large incomes. Many Americans are uninsured or stuck with medical debt, and the steep costs of even basic treatments in the American medical system likely hurt efforts to get everybody on board with a public health measure.^[Recall that many people received bills for coronavirus tests that were supposed to be free.] Furthermore, poor Americans likely interact less with medical care providers, only in situations of urgency, and a preventative vaccine simply might not seem urgent to broad swathes of America.

A potential future direction for this research would be to explore the same problem through the lens of regression. Could we accurately predict the percentage of fully-vaccinated individuals in a given county using our features? Perhaps, but the regression problem is more difficult and less useful for a variety of reasons, mostly pertaining to the nature of the response variable. Firstly, the scale of the response variable (%-vaccinated) changes over time, hindering model interpretability (e.g., linear regression coefficients will no longer be useful as overall vaccination rates increase). Secondly, with our goal of identifying vaccine-hesitant counties in mind, the utility of predicting raw percentages seems to diminish. We don't care much about exactly what percentage of residents are vaccinated in a particular county, but rather whether or not a particular county is lagging behind the rest of the country, a task that suggests a transformation of the response variable and is much more suited to our binary classification framework. 

Our inferences on which features are most useful in predicting vaccine hesitancy suggests potential avenues for societal intervention in health messaging. For example, our analysis has clearly demonstrated that constituents' vaccine decisions tend to be highly related to their political leanings. Signaling from party leaders on both sides (particularly Republican) may thus be crucial factors in enabling the country to reach widespread vaccination status. Additionally, our finding relating the percentage of Black residents in a county to vaccination rates suggests that more could be done to effectively communicate the safety, efficacy, and no-cost nature of the vaccine to minority communities.
